
```python
import sys
from fastrtc import ReplyOnPause, Stream, get_stt_model, get_tts_model
from loguru import logger
try:
    from backend.nano import NanoAgent
except ImportError:
    from nano import NanoAgent

# 1. Initialize Local Models (Speed)
# These run locally inside the Docker container using the libraries installed in Dockerfile
# Moonshine (STT) and Kokoro (TTS) are SOTA for speed.
stt_model = get_stt_model()
tts_model = get_tts_model()

# 2. Initialize the Voice Brain (Nano Agent)
# This connects to Gemini Flash Lite for "Thinking"
vhf_brain = NanoAgent(
    name="Ada.VHF",
    system_instruction="""
    ROL: West Istanbul Marina (WIM) VHF Telsiz OperatÃ¶rÃ¼.
    Ä°SÄ°M: Ada.
    KANAL: 72.
    
    KURALLAR:
    1. KÄ±sa, net ve denizcilik jargonuna (SMCP) uygun konuÅŸ.
    2. CevaplarÄ±nÄ± TÃ¼rkÃ§e ver (Ä°stanbul TÃ¼rkÃ§esi).
    3. Asla emoji veya markdown kullanma. Sadece dÃ¼z metin.
    4. CÃ¼mlelerini "Tamam" (Over) ile bitir.
    5. Asla matematik hesabÄ± yapma, sadece operasyonel bilgi ver.
    """
)

# Logging config
logger.remove(0)
logger.add(sys.stderr, level="DEBUG")

def echo(audio):
    """
    The Voice Loop: Audio -> Text -> LLM -> Text -> Audio
    """
    # 1. Hear (Local STT)
    # Audio comes as (sample_rate, numpy_array)
    transcript = stt_model.stt(audio)
    
    # Filter out empty noise
    if not transcript or len(transcript.strip()) < 2: return
    
    logger.debug(f"ðŸŽ¤ Heard: {transcript}")
    
    # 2. Think (Cloud LLM)
    response_text = vhf_brain.chat(transcript)
    logger.debug(f"ðŸ¤– Spoke: {response_text}")
    
    # 3. Speak (Local TTS)
    # Streams audio chunks back to the browser via WebRTC
    for chunk in tts_model.stream_tts_sync(response_text):
        yield chunk

# 3. Initialize Stream
# Mode "send-receive" is crucial for 2-way audio
stream = Stream(
    ReplyOnPause(echo),
    modality="audio",
    mode="send-receive",
    ui_args={"title": "Ada VHF Radio (Channel 72)"}
)
```
