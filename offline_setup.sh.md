
# ðŸ”ï¸ Ada Stargate: Mountain Mode Setup (Survival Kit)

Bu script, Ada sistemini **internet baÄŸlantÄ±sÄ± olmayan** ortamlarda (aÃ§Ä±k deniz, gÃ¼venli sÄ±ÄŸÄ±nak, daÄŸ evi) Ã§alÄ±ÅŸtÄ±rmak iÃ§in gerekli yapay zeka modellerini Ã¶nceden indirir.

**KullanÄ±m:** Ä°nternet baÄŸlantÄ±nÄ±z varken bu scripti bir kez Ã§alÄ±ÅŸtÄ±rÄ±n.

---

```bash
#!/bin/bash

# Renkler
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${BLUE}"
echo "    _    ____      _         "
echo "   / \  |  _ \    / \        "
echo "  / _ \ | | | |  / _ \       "
echo " / ___ \| |_| | / ___ \      "
echo "/_/   \_\____/ /_/   \_\     "
echo -e "${NC}"
echo -e "${GREEN}>>> ADA STARGATE: SURVIVAL KIT INSTALLER <<<${NC}"
echo "---------------------------------------------"
echo "Hedef: Tam Ã‡evrimdÄ±ÅŸÄ± Operasyon (Mountain Mode)"
echo "---------------------------------------------"

# 1. Docker KontrolÃ¼
if ! command -v docker &> /dev/null
then
    echo -e "${RED}[HATA] Docker bulunamadÄ±. LÃ¼tfen Ã¶nce Docker Desktop'Ä± kurun.${NC}"
    exit 1
fi

# 2. Servisleri BaÅŸlat
echo -e "\n${BLUE}[1/4] AltyapÄ± baÅŸlatÄ±lÄ±yor (Docker)...${NC}"
docker-compose -f docker-compose.hyperscale.yml up -d ada-local-llm ada-redis ada-qdrant

echo "Servislerin Ä±sÄ±nmasÄ± bekleniyor (10sn)..."
sleep 10

# 3. Yerel LLM Ä°ndirme (Ollama - Gemma 2B)
echo -e "\n${BLUE}[2/4] Yerel Beyin (Gemma 2B) indiriliyor...${NC}"
echo "Bu iÅŸlem internet hÄ±zÄ±nÄ±za baÄŸlÄ± olarak zaman alabilir (1.5 GB)."

if docker exec -it ada_local_llm ollama list | grep -q "gemma:2b"; then
    echo -e "${GREEN}[OK] Gemma 2B zaten yÃ¼klÃ¼.${NC}"
else
    docker exec -it ada_local_llm ollama pull gemma:2b
    if [ $? -eq 0 ]; then
        echo -e "${GREEN}[OK] Yerel Beyin hazÄ±r.${NC}"
    else
        echo -e "${RED}[HATA] Model indirilemedi.${NC}"
        exit 1
    fi
fi

# 4. Embedding Modeli Ä°ndirme (RAG iÃ§in)
# Python container iÃ§inde huggingface modelini cache'e Ã§eker.
echo -e "\n${BLUE}[3/4] HafÄ±za Modelleri (Embeddings) Ã¶nbelleÄŸe alÄ±nÄ±yor...${NC}"

docker exec -it ada_core_hyperscale python -c "
from langchain_community.embeddings import HuggingFaceEmbeddings
print('Downloading all-MiniLM-L6-v2...')
try:
    embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
    print('SUCCESS: Model cached locally.')
except Exception as e:
    print(f'ERROR: {e}')
"

# 5. DoÄŸrulama
echo -e "\n${BLUE}[4/4] Sistem DoÄŸrulamasÄ±...${NC}"

# Test Ollama
RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate -d '{
  "model": "gemma:2b",
  "prompt": "Say System Online",
  "stream": false
}')

if [[ $RESPONSE == *"System Online"* ]]; then
    echo -e "${GREEN}[TEST] Yerel Zeka: AKTÄ°F${NC}"
else
    echo -e "${RED}[TEST] Yerel Zeka: YANIT YOK${NC}"
fi

echo -e "\n---------------------------------------------"
echo -e "${GREEN}âœ… MOUNTAIN MODE HAZIR${NC}"
echo "ArtÄ±k internet kablosunu Ã§ekebilirsiniz."
echo "Ada yerel zeka ile Ã§alÄ±ÅŸmaya devam edecektir."
echo "---------------------------------------------"
```
